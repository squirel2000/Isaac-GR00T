{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "This tutorial illustrates how to fine-tune the `GR00T-N1.5` pretrained checkpoint on a post-training \n",
    "dataset using the same embodiment. This showcases the benefit of post-training, transforming a generalist model into a specialist and demonstrating a performance gain.\n",
    "\n",
    "GR00T-N1.5 openly supports the following embodiments:\n",
    "\n",
    "| Embodiment Tag | Description |\n",
    "|----------------|-------------|\n",
    "| gr1 | The GR1 dataset |\n",
    "| oxe_droid | The OxE Droid dataset |\n",
    "| agibot_genie1 | The AgiBot Genie-1 with gripper dataset |\n",
    "| new_embodiment | Any new embodiment for finetuning |\n",
    "\n",
    "> Refer to the [Embodiment Tags](../gr00t/data/embodiment_tags.py) for more details.\n",
    "\n",
    "\n",
    "For this tutorial, we will use the demo dataset `robot_sim.PickNPlace` from the [demo_data](./demo_data) folder. \n",
    "\n",
    "We will first load the pre-trained model and evaluate it on the dataset. Then we will finetune the model on the dataset and evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/gr00t/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-05 17:45:12.381636: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-05 17:45:12.381678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-05 17:45:12.382578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-05 17:45:12.387615: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-05 17:45:13.023978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from gr00t.utils.eval import calc_mse_for_single_trajectory\n",
    "import warnings\n",
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "from gr00t.model.policy import Gr00tPolicy\n",
    "from gr00t.data.schema import EmbodimentTag\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_PATH = \"nvidia/GR00T-N1.5-3B\"\n",
    "EMBODIMENT_TAG = EmbodimentTag.GR1\n",
    "DATASET_PATH = \"../demo_data/robot_sim.PickNPlace\"\n",
    "\n",
    "\n",
    "data_config = DATA_CONFIG_MAP[\"fourier_gr1_arms_only\"]\n",
    "modality_config = data_config.modality_config()\n",
    "modality_transform = data_config.transform()\n",
    "\n",
    "\n",
    "pre_trained_policy = Gr00tPolicy(\n",
    "    model_path=PRE_TRAINED_MODEL_PATH,\n",
    "    embodiment_tag=EMBODIMENT_TAG,\n",
    "    modality_config=modality_config,\n",
    "    modality_transform=modality_transform,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    modality_configs=modality_config,\n",
    "    video_backend=\"decord\",\n",
    "    video_backend_kwargs=None,\n",
    "    transforms=None,  # We'll handle transforms separately through the policy\n",
    "    embodiment_tag=EMBODIMENT_TAG,\n",
    ")\n",
    "\n",
    "\n",
    "mse = calc_mse_for_single_trajectory(\n",
    "    pre_trained_policy,\n",
    "    dataset,\n",
    "    traj_id=0,\n",
    "    modality_keys=[\"right_arm\", \"right_hand\"],   # we will only evaluate the right arm and right hand\n",
    "    steps=150,\n",
    "    action_horizon=16,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(\"MSE loss for trajectory 0:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we can see the predicted actions and the ground truth actions. The predicted actions are not perfect but they are close to the ground truth actions. That's show that the pretrained checkpoint is working well.\n",
    "\n",
    "Now let's sample 10 random trajectories and calcuate the mean MSE to get a sense of more verbose results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trajectories = len(dataset.trajectory_lengths)\n",
    "\n",
    "print(\"Total trajectories:\", total_trajectories)\n",
    "\n",
    "sampled_trajectories = np.random.choice(total_trajectories, 10)\n",
    "print(\"Sampled trajectories:\", sampled_trajectories)\n",
    "\n",
    "all_mses = []\n",
    "\n",
    "for traj_id in sampled_trajectories:\n",
    "    mse = calc_mse_for_single_trajectory(\n",
    "        pre_trained_policy,\n",
    "        dataset,\n",
    "        traj_id=traj_id,\n",
    "        modality_keys=[\"right_arm\", \"right_hand\"],   # we will only evaluate the right arm and right hand\n",
    "        steps=150,\n",
    "        action_horizon=16,\n",
    "        plot=False\n",
    "    )\n",
    "    print(f\"Trajectory {traj_id} MSE: {mse:.4f}\")\n",
    "    \n",
    "    all_mses.append(mse)\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"Mean MSE:\", np.mean(all_mses))\n",
    "print(\"Std MSE:\", np.std(all_mses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the Model\n",
    "\n",
    "Now we will finetune the model on the dataset. Without going into the details of the finetuning process, we will use the `gr00t_finetune.py` script to finetune the model. You can run the following command to finetune the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python scripts/gr00t_finetune.py \\\n",
    "    --dataset-path ./demo_data/robot_sim.PickNPlace \\\n",
    "    --num-gpus 1 \\\n",
    "    --max-steps 500 \\\n",
    "    --output-dir /tmp/gr00t-1/finetuned-model \\\n",
    "    --data-config gr1_arms_only\n",
    "```\n",
    "\n",
    "```bash\n",
    "python scripts/gr00t_finetune.py --dataset-path ./demo_data/robot_sim.PickNPlace --num-gpus 1 --max-steps 500 --output-dir /tmp/gr00t-1/finetuned-model --data-config gr1_arms_only --lora-rank 256\n",
    "```\n",
    "\n",
    "_To get a full list of the available arguments, you can run `python scripts/gr00t_finetune.py --help`._\n",
    "\n",
    "The script will save the finetuned model in the `/tmp/gr00t-1/finetuned-model` directory. We will load the finetuned model with `500` checkpoint steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Fine-tuned Model\n",
    "\n",
    "Now we can evaluate the fine-tuned model by running the policy on the dataset and see how well it performs. We will use a utility function to evaluate the policy on the dataset. This is similar to the previous tutorial in [1_pretrained_model.ipynb](1_pretrained_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.eval import calc_mse_for_single_trajectory\n",
    "import warnings\n",
    "\n",
    "finetuned_model_path = \"/tmp/gr00t-1/finetuned-model/checkpoint-500\"\n",
    "finetuned_policy = Gr00tPolicy(\n",
    "    model_path=finetuned_model_path,\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    modality_config=modality_config,\n",
    "    modality_transform=modality_transform,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "mse = calc_mse_for_single_trajectory(\n",
    "    finetuned_policy,\n",
    "    dataset,\n",
    "    traj_id=0,\n",
    "    modality_keys=[\"right_arm\", \"right_hand\"],   # we will only evaluate the right arm and right hand\n",
    "    steps=150,\n",
    "    action_horizon=16,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(\"MSE loss for trajectory 0:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaay! We have finetuned the model and evaluated it on the dataset. We can see that the model has learned the task and is able to perform the task better than the pre-trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
